{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load Open-Sourced LLMs\n",
    "\n",
    "If you're in search of an open-source alternative to ChatGPT that can be operated on your local machine, large language models (LLMs) hosted within a Jupyter Notebook offer a potent and adaptable solution.\n",
    "\n",
    "In this blog notebook, I will guide you through the installation, configuration, and utilization of open-source LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face: Your Gateway to LLMs\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) is a pivotal platform for working with large language models. It provides a wide range of pre-trained models and tools for various Natural Language Processing (NLP) tasks. Here, we are particularly interested in text generation and question answering. You can easily access these models via the Hugging Face model hub.\n",
    "\n",
    "**Getting Models from Hugging Face**\n",
    "\n",
    "To access LLMs for text generation and question answering, visit the Hugging Face model hub. You can find an extensive collection of models for different NLP tasks, including those suited for text generation and question answering.\n",
    "\n",
    "[Hugging Face Model Hub](https://huggingface.co/models)\n",
    "\n",
    "## Leaderboards and Model Licensing\n",
    "\n",
    "Hugging Face has a model leaderboard where you can explore various models for benchmarking and comparisons. Additionally, the [LLSys Leaderboard](https://llsys.ai/leaderboard) is a valuable resource for assessing the performance of large language models.\n",
    "\n",
    "When using models from Hugging Face, it's crucial to review their licenses to ensure compliance with your intended use.\n",
    "\n",
    "\n",
    "## Model Cards\n",
    "\n",
    "Hugging Face provides model cards for each model in their repository. These model cards offer detailed information about the models, including their capabilities, input formats, and performance characteristics.\n",
    "\n",
    "\n",
    "## Types of (quantized) models\n",
    "\n",
    "Large Language Models come in various flavors, each suited for different purposes. Here are some common types:\n",
    "\n",
    "- **GGUF (...):** The library is written in C/C++ for efficient inference of Llama models. It can load GGML models and run them on a **`CPU`**. You can work with GGUF models using libraries like `llama-cpp`, `ctransformers`, and the `huggingface-hub` library.\n",
    "\n",
    "- **GPTQ (...):** This quantization load and run the models using **`GPU`**. To work with GPTQ models, you'll need libraries such as `auto-gptq`, `transformers`, and `optium`.\n",
    "\n",
    "- **AWQ (...):** For AWQ models, you can explore the `autoawq` library.\n",
    "\n",
    "- **Foundational Models (Base Models):** These are the core models on which other LLMs are built. They are the foundation for various NLP tasks.\n",
    "\n",
    "This Jupyter Notebook will provide you with step-by-step instructions on how to load and utilize these different types of LLMs for your specific NLP tasks. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with foundational (base) models\n",
    "\n",
    "For this example we will work with the Zephyr model, current SOTA model under 70B of parameters: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
    "\n",
    "Dependencies:\n",
    " \n",
    "==============================================\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/transformers.git\n",
    "pip install accelerate\n",
    "```\n",
    "==============================================\n",
    "\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GGUF / GGML models (CPU based)\n",
    "\n",
    "We can use GGUF models using the `llama-cpp-python` or `ctransformers` libraries. As I am used to work with LlamaCpp I will use this one:\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "========================\n",
    "```bash\n",
    "```\n",
    "========================\n",
    "\n",
    "\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ models (GPU based)\n",
    "\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|system|>\n",
    "</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
